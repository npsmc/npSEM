{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays\n",
    "from numpy.linalg import cholesky #Return the Cholesky decomposition\n",
    "import matplotlib.pyplot as plt #plot\n",
    "\n",
    "#import os #allows you to interface with the underlying operating system that Python is running on â€“ be that Windows, Mac or Linux. \n",
    "#import pickle #serializing and de-serializing a Python object structure.\n",
    "\n",
    "# L63 and L96 models using fortran (fast)\n",
    "import models.l63f as mdl_l63\n",
    "\n",
    "# L63 and L96 models using python (slow)\n",
    "from models.L63 import l63_predict, l63_jac\n",
    "# data assimilation routines\n",
    "#from algos.EKS_EM import EKS_EM\n",
    "#from algos.CPF_BS_SEM import CPF_BS_SEM\n",
    "\n",
    "#from algos.utils import climat_background, RMSE, gen_truth, gen_obs, sampling_discrete, resampling_sys\n",
    "from methods.generate_data import generate_data\n",
    "from methods.LLR_forecasting_CV import m_LLR\n",
    "from methods.model_forecasting import m_true\n",
    "from methods.k_choice import k_choice\n",
    "from methods.CPF_BS_smoothing import _CPF_BS\n",
    "from methods.SEM import CPF_BS_SEM\n",
    "from methods.npSEM import LLR_CPF_BS_SEM\n",
    "from methods.regression_2 import regression_2\n",
    "from methods.EnKS import _EnKS\n",
    "from save_load import saveTr, loadTr\n",
    "from methods.additives import RMSE\n",
    "#from data_assimilation import data_assimilation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "### GENERATE SIMULATED DATA (LORENZ-63 MODEL)\n",
    "\n",
    "# parameters\n",
    "dx = 3 # dimension of the state\n",
    "dt_int = 0.01 # fixed integration time\n",
    "dt_model = 8 # chosen number of model time step  \\in [1, 25]-the larger dt_model the more nonliner model\n",
    "var_obs = np.array([0,1,2]) # indices of the observed variables\n",
    "dy = len(var_obs) # dimension of the observations\n",
    "H = np.eye(dx)\n",
    "#H = H[(0,2),:] #  first and third variables are observed\n",
    "h = lambda x: H.dot(x)  # observation model\n",
    "jacH = lambda x: H # Jacobian matrix  of the observation model(for EKS_EM only)\n",
    "\n",
    "\n",
    "sigma = 10.0; rho = 28.0; beta = 8.0/3 # physical parameters\n",
    "fmdl=mdl_l63.M(sigma=sigma, rho=rho, beta=beta, dtcy= dt_int)\n",
    "mx = lambda x: fmdl.integ(x) # fortran version (fast)\n",
    "jac_mx = lambda x: l63_jac(x, dt_int*dt_model, sigma, rho, beta) # python version (slow)\n",
    "\n",
    "# Setting covariances\n",
    "sig2_Q = 1; sig2_R = 2 # parameters\n",
    "Q_true = np.eye(dx) *sig2_Q # model covariance\n",
    "R_true = np.eye(dx) *sig2_R # observation covariance\n",
    "\n",
    "# prior state\n",
    "x0 = np.r_[8, 0, 30]\n",
    "\n",
    "# generate data\n",
    "T_burnin = 5*10**3\n",
    "T_train = 10*10**2 # length of the catalog\n",
    "T_test = 10*10**2 # length of the testing data\n",
    "X_train, Y_train, X_test, Y_test, yo = generate_data(x0,mx,h,Q_true,R_true,dt_int,dt_model,var_obs, T_burnin, T_train, T_test) \n",
    "X_train.time = np.arange(0,T_train)\n",
    "Y_train.time= X_train.time[1:]\n",
    "#np.random.seed(0);# random number generator\n",
    "N=np.size(Y_train.values);Ngap= np.floor(N/10); # create gaps: 10 percent of missing values\n",
    "indX=np.random.choice(np.arange(0,N), int(Ngap), replace=False);\n",
    "ind_gap_taken = divmod(indX ,len(Y_train.time));\n",
    "Y_train.values[ind_gap_taken]=np.nan;\n",
    "\n",
    "X_train0, Y_train0, X_test0, Y_test0, yo0 = generate_data(X_train.values[:,-1],mx,h,Q_true,R_true,dt_int,dt_model,var_obs, T_burnin, T_train, T_test) \n",
    "X_train0.time = np.arange(0,T_train)\n",
    "Y_train0.time= X_train0.time[1:]\n",
    "\n",
    "### PLOT STATE, OBSERVATIONS AND CATALOG\n",
    "\n",
    "# state and observations (when available)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.figure(1)\n",
    "plt.plot(X_train.values[:,1:].T,'-', color='grey')\n",
    "plt.plot(Y_train.values.T,'.k', markersize= 6)\n",
    "plt.xlabel('Lorenz-63 times')\n",
    "plt.title('Lorenz-63 true (continuous lines) and observed trajectories (points)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class estQ:\n",
    "    value = Q_true;\n",
    "    type = 'adaptive' # chosen predefined type of model error covatiance ('fixed', 'adaptive')\n",
    "    form = 'constant' # chosen esimated matrix form ('full', 'diag', 'constant')\n",
    "    base =  np.eye(dx) # for fixed base of model covariance (for 'constant' matrix form only)\n",
    "    decision = True # chosen if Q is estimated or not ('True', 'False')\n",
    "class estR:\n",
    "    value = R_true;\n",
    "    type = 'adaptive' # chosen predefined type of observation error covatiance ('fixed', 'adaptive')\n",
    "    form = 'constant' # chosen esimated matrix form ('full', 'diag', 'constant')\n",
    "    base =  np.eye(dx) # for fixed base of model covariance\n",
    "    decision = True # chosen if R is estimated or not ('True', 'False')\n",
    "class estX0:\n",
    "    decision = False # chosen if X0 is estimated or not ('True', 'False')\n",
    "    \n",
    "class estD: # for non-parametric approach only\n",
    "    decision = True # chosen if the smoothed data is updated or not ('True', 'False')\n",
    "## true_FORECASTING (dynamical model)\n",
    "m = lambda x,pos_x,ind_x,Q: m_true(x,pos_x,ind_x, Q, mx,jac_mx, dt_model)\n",
    "num_ana = 300\n",
    "data_init = np.r_['0,2,0',Y_train.values[...,:-1], Y_train.values[...,1:]];\n",
    "ind_nogap = np.where(~np.isnan(np.sum(data_init,0)))[0]; \n",
    "#  LLR_FORECASTING (non-parametric dynamical model constructed given the catalog)\n",
    "# parameters of the analog forecasting method\n",
    "class LLR:\n",
    "    class data:\n",
    "        ana = np.zeros((dx,1,len(ind_nogap))); suc = np.zeros((dx,1,len(ind_nogap)));\n",
    "        ana[:,0,:] =data_init[:dx,ind_nogap]; suc[:,0,:]  = data_init[dx:,ind_nogap]; time = Y_train.time[ind_nogap] # catalog with analogs and successors\n",
    "#        ana = X_train.values[...,:-1]; suc =X_train.values[...,1:]; time = X_train.time[:-1]#catalog with analogs and successors\n",
    "    class data_prev:\n",
    "        ana =data_init[:dx,ind_nogap]; suc = data_init[dx:,ind_nogap]; time = Y_train.time[ind_nogap] # catalog with analogs and successors\n",
    "    lag_x = 5 # lag of removed analogs around x to avoid an over-fitting forecast\n",
    "    lag_Dx = lambda Dx: np.shape(Dx)[-1]; #15 lag of moving window of analogs chosen around x \n",
    "    time_period = 1 # set 365.25 for year and \n",
    "    k_m = [] # number of analogs \n",
    "    k_Q = [] # number of analogs \n",
    "    nN_m = np.arange(20,num_ana,50) #number of analogs to be chosen for mean estimation\n",
    "    nN_Q = np.arange(20,num_ana,50) #number of analogs to be chosen for dynamical error covariance\n",
    "    lag_k = 1; #chosen the lag for k reestimation \n",
    "    estK = 'same' # set 'same' if k_m = k_Q chosen, otherwise, set 'different'\n",
    "    kernel = 'tricube'# set 'rectangular' or 'tricube'\n",
    "    k_lag = 20;\n",
    "    k_inc= 10\n",
    "    Q = estQ;\n",
    "    gam = 1;\n",
    "time = np.arange(T_train)#[np.newaxis].T\n",
    "# run the analog data assimilation\n",
    "k_m, k_Q = k_choice(LLR,LLR.data.ana,LLR.data.suc,LLR.data.time)    \n",
    "LLR.k_m =k_m; LLR.k_Q =k_Q; #LLR.lag_x = 0\n",
    "\n",
    "#xf, mean_xf, Q_xf, M_xf = m_LLR(X_test.values[:,:-1],1,np.ones([1]),LLR)\n",
    "#xft, mean_xft,Q_xft, M_xft =  m(X_test.values[:,:-1],1,np.ones([1]),Q_true)\n",
    "#np.sqrt(np.mean((mean_xft-X_test.values[:,1:])**2))\n",
    "#np.sqrt(np.mean((mean_xf-X_test.values[:,1:])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_init1 =  np.r_['0,2,0',X_train.values[...,1:-1], X_train.values[...,2:]];\n",
    "# LLR.data.ana[:,0,:] = data_init1[:dx,ind_nogap]; LLR.data.suc[:,0,:] = data_init1[dx:,ind_nogap]\n",
    "# k_m, k_Q = k_choice(LLR,LLR.data.ana,LLR.data.suc,LLR.data.time)    \n",
    "# LLR.k_m =k_m; LLR.k_Q =k_Q; #LLR.lag_x = 0\n",
    "# k_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLR.lag_x=0\n",
    "xf, mean_xff, Q_xf, M_xf = m_LLR(X_test.values[:,:-1],1,np.ones([1]),LLR)\n",
    "time = np.arange(0,T_test-1)\n",
    "B = Q_true\n",
    "xb= X_test.values[...,0]\n",
    "N_iter = 100\n",
    "Nf = 10# number of particles\n",
    "Ns =5 # number of realizations\n",
    "from tqdm import tqdm\n",
    "\n",
    "X_conditioning = np.zeros([dx,T_test]); err_smo= 0; err_fil= 0\n",
    "m_hat = lambda  x,pos_x,ind_x: m_LLR(x,pos_x,ind_x,LLR)    \n",
    "for j in tqdm(range(N_iter)):\n",
    "    if j==0:\n",
    "        Xss,_,_= _EnKS(dx, 20, len(X_test.time[1:]), H, R_true, Y_test.values,X_test.values, dy, xb, B, LLR.Q.value, 1, m_hat)\n",
    "        X_conditioning = np.mean(Xss,1);\n",
    "    Xs,Xa,Xf,_,_,loglik = _CPF_BS(Y_test.values,X_test.values,m_hat, Q_true, H, R_true,xb,B, X_conditioning,dx, Nf, Ns,time)\n",
    "    X_conditioning = Xs[:,-1,:];\n",
    "    \n",
    "    if j >5:\n",
    "        err_smo +=  ((Xs.mean(1)-X_test.values)**2);   err_fil +=  ((Xa.mean(1)-X_test.values)**2); \n",
    "print('smo={}, fil={}'.format(np.sqrt(np.mean(err_smo)/N_iter), np.sqrt(np.mean(err_fil)/N_iter)))\n",
    "print(RMSE(mean_xff-X_test.values[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% (SEM): STOCHASTIC EXPECATION-MAXIMIZATION  vs (npSEM): NON-PARAMETRIC STOCHASTIC EXPECATION-MAXIMIZATION \n",
    "# SETTING PARAMETERS \n",
    "N_iter =100# number of iterations of EM algorithms\n",
    "# Step functions\n",
    "gam1 = np.ones(N_iter,dtype=int)  #for SEM (stochastic EM)\n",
    "gam2 = np.ones(N_iter, dtype=int)\n",
    "for k in range(50,N_iter):\n",
    "    gam2[k] = k**(-0.7) # for SAEM (stochastic approximation EM)\n",
    "#gam1 = gam2\n",
    "\n",
    "rep = 1 # number of repetitions for each algorithm\n",
    "X_conditioning = np.zeros([dx,T_train+1]) # the conditioning trajectory (only for CPF-BS-SEM and CPF-AS-SEM)\n",
    "B = Q_true\n",
    "xb= X_train.values[...,0]\n",
    "# initial parameters\n",
    "aintQ = 0.5; bintQ = 5\n",
    "aintR = 1; bintR = 5\n",
    "    \n",
    "Nf = 10# number of particles\n",
    "Ns = 5 # number of realizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_init = .5*np.eye(dy)    \n",
    "Q_init = 3*np.eye(dx)\n",
    "LLR.Q.value = Q_init\n",
    "estD.decision = True;  LLR.lag_x =5;  LLR.Q = estQ;\n",
    "LLR.data.ana = np.zeros((dx,1,len(ind_nogap))); LLR.data.suc = np.zeros((dx,1,len(ind_nogap)));\n",
    "LLR.data.ana[:,0,:] =data_init[:dx,ind_nogap]; LLR.data.suc[:,0,:] = data_init[dx:,ind_nogap]; LLR.data.time = Y_train.time[ind_nogap] \n",
    "\n",
    "LLR.Q.value = Q_init\n",
    "k_m, k_Q = k_choice(LLR,LLR.data.ana,LLR.data.suc,LLR.data.time)    \n",
    "LLR.k_m =k_m; LLR.k_Q =k_Q;\n",
    "\n",
    "m_hat = lambda  x,pos_x,ind_x: m_LLR(x,pos_x,ind_x,LLR)\n",
    "Xs, _, _ = _EnKS(dx, 20, len(time[1:]), H, R_init, Y_train.values,X_train.values, dy, xb, B, Q_init, 1, m_hat)\n",
    "X_conditioning = np.squeeze(Xs.mean(1))\n",
    "\n",
    "out_npSEM_adap = LLR_CPF_BS_SEM(Y_train.values,X_train.values, LLR, H, R_init,xb,B, X_conditioning,dx, Nf, Ns,X_train.time, N_iter, gam1, estD, estQ, estR, estX0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
